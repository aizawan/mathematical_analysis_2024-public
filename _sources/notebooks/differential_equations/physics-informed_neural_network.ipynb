{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20301ee0",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Network\n",
    "解析的に解ける微分方程式は一部であり，大半の微分方程式は有限回の積分と変形で解けない微分方程式であった．このような解析的に解けない微分方程式は数値計算のアルゴリズムを用いて解が計算されるが，一般的に膨大な計算リソースを必要とする．この問題への解決への糸口として，常微分方程式や偏微分方程式で記述される物理法則の近似解を予測するニューラルネットワークである **Physics Informed Neural Network（PINN）** が注目を集めている．PINNsは求めたい関数 $f$ の入出力関係 $y=f(x)$ を **多層パーセプトロン（Multi Layer Perceptron; MLPs）** でモデル化し，与えれた微分方程式・初期条件・境界条件から定義される誤差関数の最小化問題から関数 $f$ を学習する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba98e98",
   "metadata": {},
   "source": [
    "## PINNsによる熱伝導方程式の解の計算\n",
    "熱伝導方程式の解をPINNsで学習する．熱伝導方程式は均質の物質で構成されている長さ $L$ の棒について，時刻 $t$ における位置 $x$ の温度分布 $u(x,t)$ を記述する偏微分方程式であった．熱伝導方程式（拡散方程式）は以下の式で与えられた．\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = a^2 \\frac{\\partial^2 u}{\\partial x^2}, a>0\n",
    "$$\n",
    "\n",
    "また棒の両端（定義域の端）の温度分布の制約として次の境界条件を設定する．\n",
    "\n",
    "$$\n",
    "u(0,t)=u(L,t)=0\n",
    "$$\n",
    "\n",
    "また時刻 $t=0$ の温度分布として初期条件は以下で考える．\n",
    "\n",
    "$$\n",
    "u(x,0) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(x-\\mu)^2}{2 \\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "この方程式は変数分離の仮定とフーリエ級数展開の利用から特殊解を導出できるが，ここではこの微分方程式の解をMLPsでモデル化したPINNsで学習する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a2160",
   "metadata": {},
   "source": [
    "### ライブラリのインポート\n",
    "まずはじめに必要なライブラリをインポートする．今回は `Numpy` と `Matplotlib` に加えて `PyTorch` と呼ばれる深層学習ライブラリを利用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd912c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad6ff8",
   "metadata": {},
   "source": [
    "また今回利用した `PyTorch` のバージョンは以下である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96670d56",
   "metadata": {},
   "source": [
    "### データの作成\n",
    "まず学習データ，境界条件，初期条件，プロット用のデータを作成する．各データはNumpyで一様分布に従ってランダム生成し，Pytorchで計算できるように`torch.tensor` を使って `Tensor`型へ変換する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c75b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "L = 2.0\n",
    "N, N_bc, N_ic = 1000, 100, 100\n",
    "\n",
    "# 学習データ\n",
    "N = 1000\n",
    "x = torch.tensor(\n",
    "    np.random.uniform(0, L, (N, 1)), dtype=torch.float32, requires_grad=True)\n",
    "t = torch.tensor(\n",
    "    np.random.uniform(0, 1, (N, 1)), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# 境界条件\n",
    "N_bc = 100\n",
    "x_bc = torch.tensor([0, L], dtype=torch.float32).view(-1, 1)\n",
    "t_bc = torch.tensor([0, 0], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 初期条件\n",
    "N_ic = 100\n",
    "x_ic = torch.tensor(np.random.uniform(0, L, (N_ic, 1)), dtype=torch.float32)\n",
    "t_ic = torch.tensor([0]*N_ic, dtype=torch.float32).view(-1, 1)\n",
    "u_ic = (1. / torch.sqrt(torch.tensor(2*np.pi*0.1))) * torch.exp(-(x_ic-1)**2 / (2*0.1))\n",
    "\n",
    "# プロット用のデータ\n",
    "N_plot = 100\n",
    "xx_plot, tt_plot = np.meshgrid(\n",
    "    np.linspace(0, L, N_plot), np.linspace(0, 1, N_plot))\n",
    "xx_plot = torch.tensor(xx_plot.reshape(-1, 1), dtype=torch.float32)\n",
    "tt_plot = torch.tensor(tt_plot.reshape(-1, 1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fd938",
   "metadata": {},
   "source": [
    "各データの形状は `.shape` でみたときに (データ数, データの次元)となるように `view` メソッドを使って整形している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cebb5b",
   "metadata": {},
   "source": [
    "実際の関数の出力結果の値が定義されているのは初期条件のデータのみに注意されたい．そのため今回考える初期条件を可視化しておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a181406",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_ic.numpy(), u_ic.numpy())\n",
    "plt.title('initial condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb66ae",
   "metadata": {},
   "source": [
    "### PINNsの定義\n",
    "続いて変数 $x,t$ を入力とし解 $u(x,t)$ を出力するニューラルネットワークを構築する．Pytorchでは `torch.nn.Module` を継承したクラスを用いてニューラルネットワークを定義する．簡単に解説すると，`__init__` メソッド内で線形変換を行う **全結合層** `torch.nn.Linear` を3層定義している．全結合層の引数はそれぞれ入力次元, 出力次元を表す．\n",
    "\n",
    "`forward` メソッドはニューラルネットワークの **順伝播（forward propagation）** を定義する．メソッドの引数は関数への入力 $x,t$ を与え，モデル化した関数の出力 $y$ を返す．`forward` ではまず `cat` 関数で `x` と `t` の値を結合し，定義した全結合層に伝播している．各全結合層の出力は **tanh関数（Hyperbolic tangent function; 双曲線正接関数）** によって非線形変換される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(torch.nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=32, output_dim=1):\n",
    "        super(PINN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        h = torch.tanh(self.fc1(inputs))\n",
    "        h = torch.tanh(self.fc2(h))\n",
    "        y = self.fc3(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26413f0",
   "metadata": {},
   "source": [
    "構築したニューラルネットワークをインスタンス化する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0de5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = PINN()\n",
    "optimizer = torch.optim.Adam(f.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76cfd1e",
   "metadata": {},
   "source": [
    "### PINNsの誤差関数\n",
    "では，与えられた熱伝導方程式解くために，偏微分方程式に関する誤差（`pde_loss`），初期条件に関する誤差（`ic_loss`），境界条件に関する誤差（`bc_loss`）を次のように定義する．`torch.autograd.grad` で引数として与えた `outputs` を `inputs` で微分したときの値を計算できる．そのため，`u_t` と `u_x` はそれぞれ $u(x,t)$ の $x$ と $t$ に関する一階微分を表す．一方で `u_xx` は $x$ の二階微分を表す．\n",
    "\n",
    "`pde_loss` は与えられた微分方程式に関する誤差である．セル内の `pde_loss = torch.mean((u_t - alpha * u_xx)**2)` が対応している．これの式の意味は与えられた微分方程式\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} - a^2 \\frac{\\partial^2 u}{\\partial x^2} = 0\n",
    "$$\n",
    "\n",
    "の右辺が左辺と等しくなるように，つまり右辺が $0$ になるような平均二乗誤差を示している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pde_loss(f, x, t, alpha):\n",
    "    u = f(x, t)\n",
    "    u_t = torch.autograd.grad(outputs=u, inputs=t, \n",
    "                              grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(outputs=u, inputs=x, \n",
    "                              grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(outputs=u_x, inputs=x, \n",
    "                               grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
    "    pde_loss = torch.mean((u_t - alpha * u_xx)**2)\n",
    "    return pde_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5601b719",
   "metadata": {},
   "source": [
    "初期条件の誤差 `ic_loss` は同様に初期条件と予測結果が一致するとき最小の値をとる平均二乗誤差を表す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ic_loss(f, x_ic, t_ic, u_ic):\n",
    "    u_ic_pred = f(x_ic, t_ic)\n",
    "    ic_loss = torch.mean((u_ic - u_ic_pred)**2)\n",
    "    return ic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef0418",
   "metadata": {},
   "source": [
    "境界条件についても同様である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bc_loss(f, x_bc, t_bc):\n",
    "    u_bc_pred = f(x_bc, t_bc)\n",
    "    bc_loss = torch.mean(u_bc_pred**2)\n",
    "    return bc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee2b56",
   "metadata": {},
   "source": [
    "### PINNsの学習\n",
    "続いて，定義したPINNsを定義した誤差が最小になるように **最急降下法（Gradient Descent）** で学習する．最急降下法は与えられたパラメータ $\\theta$ を持つモデル $f$ （今回はMLPs）について誤差の **勾配（Gradient）** を計算し，勾配の逆方向にパラメータを更新することで，誤差関数を最小化するパラメータを求めるアルゴリズムである．最急降下法は基本的には，パラメータの初期化（`optimizer.zero_grad()`），誤差関数の値を計算（`loss = pde_loss + ic_loss + bc_loss`），勾配を計算（`loss.backward()`），パラメータの更新（`optimizer.step()`）の4ステップからなる．ここで現れる `optimizer` はモデルのパラメータを管理・更新するためのクラスのようなものである．\n",
    "\n",
    "このステップを `N_iter` 回繰り返す．以下が実装である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02607dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "N_iter = 3000\n",
    "for i in range(N_iter):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pde_loss = calc_pde_loss(f, x, t, alpha)\n",
    "    ic_loss = calc_ic_loss(f, x_ic, t_ic, u_ic)\n",
    "    bc_loss = calc_bc_loss(f, x_bc, t_bc)\n",
    "\n",
    "    loss = pde_loss + ic_loss + bc_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}, Loss {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d5e2f",
   "metadata": {},
   "source": [
    "誤差の値をprintしてみてもわかるように誤差関数の値が減少し，熱伝導方程式とその条件をうまく学習できていることがわかる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be14c4f",
   "metadata": {},
   "source": [
    "### PINNsの出力の可視化\n",
    "最後に学習されたPINNsの出力結果を確認する．基本的には第13回でのプロットと同じであるが，勾配を計算しないように `with torch.no_grad()` を呼び出していることに注意されたい．このスケールのニューラルの学習では無視できるが大きなニューラルネットワークを学習するときに勾配計算は非常に重く，メモリを要する処理である．`torch.no_grad()` は勾配計算をしないようにする処理である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u_pred = f(xx_plot, tt_plot)\n",
    "    u_pred = u_pred.reshape(N_plot, N_plot).numpy()\n",
    "    plt.imshow(u_pred, cmap='magma')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Iteration {N_iter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64bb88",
   "metadata": {},
   "source": [
    "結果をみると熱の広がりを表す解が学習から獲得できていることがわかる．以上より，PINNsと呼ばれるニューラルネットワークを用いて熱伝導方程式の解を学習から発見することができた．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
